{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add train and test files please\n",
    "lee_train_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/04082020_CengizHocaTalebi/Random_Articles_Full_Texts.txt\"\n",
    "lee_test_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/04082020_CengizHocaTalebi/Random_Articles_Full_Texts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization process\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from gensim.utils import lemmatize\n",
    "#from pattern.en import lemma\n",
    "\n",
    "def lemmat(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "\n",
    "        for i, line in enumerate(f):\n",
    "            #tokens = gensim.utils.lemmatize(line,allowed_tags=re.compile('(NN|VB|JJ|RB)'), light=False, stopwords=frozenset({}), min_length=2, max_length=15)\n",
    "            tokens=[wd.decode('utf-8').split('/')[0] for wd in lemmatize(line)]\n",
    "            #tokens=[wd.decode('utf-8').split('/')[0] for wd in lemma(line)]\n",
    "            if tokens_only:\n",
    "                #yield str(tokens)\n",
    "                yield list(tokens)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield TaggedDocument(tokens, [i])\n",
    "                #yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = list(lemmat(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how test corpus look like after lemmatization. It creates a txt file\n",
    "    \n",
    "with open('C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/04082020_CengizHocaTalebi/test_corpus_list_to_text.txt', 'w') as f:\n",
    "    for item in test_corpus:\n",
    "        #f.write(\"{0}\\n\".format(item))\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_demo_corpus = list(lemmat(lee_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how train corpus look like after lemmatization. It creates a txt file\n",
    "    \n",
    "with open('C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/train_corpus_list_to_text.txt', 'w') as f:\n",
    "    for item in train_demo_corpus:\n",
    "        f.write(\"{0}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this part for just to change the test file for lemmat option\n",
    "lee_test_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/04082020_CengizHocaTalebi/Test_EURADWASTE_ChemACS_ChemPC_PhilIAFOR_PsyIAFOR_PsyINPACT_CompFEDCIS.txt\"\n",
    "test_corpus = list(lemmat(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not preprocessing required, use this part:\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "                \n",
    "train_demo_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the types\n",
    "print(type(test_corpus[0]))\n",
    "print(type(test_corpus[:]))\n",
    "print(type(train_demo_corpus[0]))\n",
    "print(type(train_demo_corpus[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_demo_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate a Doc2Vec Object\n",
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=2, epochs=40)\n",
    "\n",
    "#mymodel_1: for model.save\n",
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=12, dm_concat=1, dbow_words=1)\n",
    "\n",
    "#mymodel_2: for model.save\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Vocabulary\n",
    "model.build_vocab(train_demo_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to Train\n",
    "%time model.train(train_demo_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/mymodel_2_train_18000_plus_1random_test_cogsci_confs_plus_1random.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/mymodel_2_train_18000_plus_1random_test_cogsci_confs_plus_1random.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferring a Vector\n",
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing Model\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_demo_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_demo_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's count how each document ranks with respect to the training corpus\n",
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing the Model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = []\n",
    "\n",
    "for doc_id in range(32400):\n",
    "    reg_list_real=[]\n",
    "    reg_list_ideal=[]\n",
    "    \n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    #print (sims)\n",
    "    a, b = zip(*sims)\n",
    "    \n",
    "    \n",
    "    reg_list_real=list(b)\n",
    "    \n",
    "    max= 0\n",
    "    for i in reg_list_real:\n",
    "        if i > max:\n",
    "            max=i\n",
    "      \n",
    "    reg_list_ideal=[0] * 5\n",
    "    \n",
    "    reg_list_ideal.insert(0,100)\n",
    "    \n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    slope, intercept, r_value, p_value, std_err=stats.linregress(reg_list_real,reg_list_ideal)\n",
    "    #print(\"p_value: %f\" % (p_value))\n",
    "    \n",
    "    \n",
    "    d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),\"P Value\":p_value})\n",
    "\n",
    "    \n",
    "     \n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col4 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col3 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col6 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col0 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col5 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col5 {\n",
       "            background-color:  yellow;\n",
       "        }    #T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col4 {\n",
       "            background-color:  yellow;\n",
       "        }</style>  \n",
       "<table id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "        <th class=\"col_heading level0 col1\" >1</th> \n",
       "        <th class=\"col_heading level0 col2\" >2</th> \n",
       "        <th class=\"col_heading level0 col3\" >3</th> \n",
       "        <th class=\"col_heading level0 col4\" >4</th> \n",
       "        <th class=\"col_heading level0 col5\" >5</th> \n",
       "        <th class=\"col_heading level0 col6\" >6</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col0\" class=\"data row0 col0\" >0.225666</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col1\" class=\"data row0 col1\" >0.252306</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col2\" class=\"data row0 col2\" >0.123591</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col3\" class=\"data row0 col3\" >0.277512</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col4\" class=\"data row0 col4\" >0.324766</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col5\" class=\"data row0 col5\" >0.0752154</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row0_col6\" class=\"data row0 col6\" >0.249792</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col0\" class=\"data row1 col0\" >0.204737</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col1\" class=\"data row1 col1\" >0.262342</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col2\" class=\"data row1 col2\" >0.0878656</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col3\" class=\"data row1 col3\" >0.459776</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col4\" class=\"data row1 col4\" >0.26893</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col5\" class=\"data row1 col5\" >0.182366</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row1_col6\" class=\"data row1 col6\" >0.429132</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col0\" class=\"data row2 col0\" >0.240892</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col1\" class=\"data row2 col1\" >0.231798</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col2\" class=\"data row2 col2\" >0.173145</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col3\" class=\"data row2 col3\" >0.461738</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col4\" class=\"data row2 col4\" >0.429311</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col5\" class=\"data row2 col5\" >0.11021</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row2_col6\" class=\"data row2 col6\" >0.69768</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col0\" class=\"data row3 col0\" >0.567484</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col1\" class=\"data row3 col1\" >0.438037</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col2\" class=\"data row3 col2\" >0.3619</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col3\" class=\"data row3 col3\" >0.307437</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col4\" class=\"data row3 col4\" >0.338041</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col5\" class=\"data row3 col5\" >0.34262</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row3_col6\" class=\"data row3 col6\" >0.238862</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row4\" class=\"row_heading level0 row4\" >4</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col0\" class=\"data row4 col0\" >0.253152</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col1\" class=\"data row4 col1\" >0.292074</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col2\" class=\"data row4 col2\" >0.191294</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col3\" class=\"data row4 col3\" >0.289149</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col4\" class=\"data row4 col4\" >0.103051</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col5\" class=\"data row4 col5\" >0.47068</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row4_col6\" class=\"data row4 col6\" >0.0729354</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row5\" class=\"row_heading level0 row5\" >5</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col0\" class=\"data row5 col0\" >0.375301</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col1\" class=\"data row5 col1\" >0.366288</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col2\" class=\"data row5 col2\" >0.218967</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col3\" class=\"data row5 col3\" >0.330278</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col4\" class=\"data row5 col4\" >0.173994</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col5\" class=\"data row5 col5\" >0.524088</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row5_col6\" class=\"data row5 col6\" >0.13181</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467level0_row6\" class=\"row_heading level0 row6\" >6</th> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col0\" class=\"data row6 col0\" >0.213411</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col1\" class=\"data row6 col1\" >0.253042</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col2\" class=\"data row6 col2\" >0.226135</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col3\" class=\"data row6 col3\" >0.180269</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col4\" class=\"data row6 col4\" >0.635897</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col5\" class=\"data row6 col5\" >0.115171</td> \n",
       "        <td id=\"T_65ef5352_31d9_11eb_ae9b_9ca9b39de467row6_col6\" class=\"data row6 col6\" >0.453651</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a41da0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the Model without P values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = []\n",
    "\n",
    "for doc_id in range(7):\n",
    "    reg_list_real=[]\n",
    "    reg_list_ideal=[]\n",
    "    \n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    #print (sims)\n",
    "    a, b = zip(*sims)\n",
    "    \n",
    "        \n",
    "    d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),a[6]:(b[6])})\n",
    "    #d.append({a[0]:(b[0])})\n",
    "    #d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),a[6]:(b[6]),a[7]:(b[7]),a[8]:(b[8]),a[9]:(b[9]),a[10]:(b[10]),a[11]:(b[11]),a[12]:(b[12]),a[13]:(b[13]),a[14]:(b[14]),a[15]:(b[15]),a[16]:(b[16]),a[17]:(b[17]),a[18]:(b[18]),a[19]:(b[19]),a[20]:(b[20]),a[21]:(b[21]),a[22]:(b[22]),a[23]:(b[23]),a[24]:(b[24]),a[25]:(b[25]),a[26]:(b[26]),a[27]:(b[27]),a[28]:(b[28]),a[29]:(b[29]),a[30]:(b[30]),a[31]:(b[31]),a[32]:(b[32]),a[33]:(b[33]),a[34]:(b[34]),a[35]:(b[35]),a[36]:(b[36]),a[37]:(b[37]),a[38]:(b[38]),a[39]:(b[39]),a[40]:(b[40]),a[41]:(b[41]),a[42]:(b[42]),a[43]:(b[43]),a[44]:(b[44]),a[45]:(b[45]),a[46]:(b[46]),a[47]:(b[47]),a[48]:(b[48]),a[49]:(b[49])})\n",
    "     \n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export above table as csv file\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1).to_excel(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/mymodel_2_train_18000_plus_1random_test_cogsci_confs_plus_Euradwaste13_Phil16_Phil12_Comp16_Psy16_Chem16.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be sure about the doc number and text matching\n",
    "for f in range(1):\n",
    "    print('Test Document ({}): «{}»\\n'.format(f, ' '.join(test_corpus[f][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
