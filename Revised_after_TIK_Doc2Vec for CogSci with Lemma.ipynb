{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_train_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/SubFields/TrainSet_Subfields/TrainSet_Subfields.txt\"\n",
    "lee_test_file=\"C:/Users1/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/2020/txt/cogsci_2020_lemmatized_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization process\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "iteration_count = [0]\n",
    "from gensim.utils import lemmatize\n",
    "#from pattern.en import lemma\n",
    "\n",
    "def lemmat(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        print (iteration_count[0])\n",
    "        for i, line in enumerate(f):\n",
    "            #tokens = gensim.utils.lemmatize(line,allowed_tags=re.compile('(NN|VB|JJ|RB)'), light=False, stopwords=frozenset({}), min_length=2, max_length=15)\n",
    "            tokens=[wd.decode('utf-8').split('/')[0] for wd in lemmatize(line)]\n",
    "            #tokens=[wd.decode('utf-8').split('/')[0] for wd in lemma(line)]\n",
    "            iteration_count[0]+=1\n",
    "            print (iteration_count[0])\n",
    "            if tokens_only:\n",
    "                #yield str(tokens)\n",
    "                yield list(tokens)\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield TaggedDocument(tokens, [i])\n",
    "                #yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "test_corpus = list(lemmat(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how test corpus look like after lemmatization. It creates a txt file\n",
    "    \n",
    "with open('C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/2020/txt/cogsci_2020_lemmatized_text.txt', 'w') as f:\n",
    "    for item in test_corpus:\n",
    "        #f.write(\"{0}\\n\".format(item))\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "train_demo_corpus = list(lemmat(lee_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check how train corpus look like after lemmatization. It creates a txt file\n",
    "    \n",
    "with open('C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/Train26Subfields_lemmatized.txt', 'w') as f:\n",
    "    for item in train_demo_corpus:\n",
    "        f.write(\"{0}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this part for just to change the test file for lemmat option\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "lee_test_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/2020/txt/test_sets_400.txt\"\n",
    "test_corpus = list(lemmat(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not preprocessing required, use this part:\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        \n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "#lee_train_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/Train_6Fields108KArticles.txt\"\n",
    "#train_demo_corpus = list(read_corpus(lee_train_file))\n",
    "lee_test_file=\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/2020/txt/cogsci_2020.txt\"\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the types\n",
    "print(type(test_corpus[0]))\n",
    "print(type(test_corpus[:]))\n",
    "print(type(train_demo_corpus[0]))\n",
    "print(type(train_demo_corpus[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_corpus[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(train_demo_corpus[107999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covers the following three steps in one block\n",
    "#You can change the parameters and re-run this block to see the results for different parameters.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=300, window=8, min_count=3, epochs=20, dm_mean=0, dm_concat=1, dbow_words=0)\n",
    "model.build_vocab(train_demo_corpus)\n",
    "%time model.train(train_demo_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#Instantiate a Doc2Vec Object\n",
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=2, epochs=40)\n",
    "\n",
    "#mymodel_1: for model.save\n",
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=12, dm_concat=1, dbow_words=1)\n",
    "\n",
    "#mymodel_2: for model.save\n",
    "#model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=2, epochs=40, dm=1, window=8)\n",
    "\n",
    "#mymodel_3: for model.save\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=2, epochs=40, dm=1, window=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Vocabulary\n",
    "model.build_vocab(train_demo_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to Train\n",
    "%time model.train(train_demo_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/mymodel_15_lemma_train_26Subfield.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "model = Doc2Vec.load(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/Final_Doc2Vec_Model_to_Import/mymodel_15_lemma_train_18KRnd_test_confs.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.infer_vector(train_demo_corpus[0].words)\n",
    "inferred_vector=model.infer_vector(test_corpus[0])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=1)\n",
    "print (sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inferring a Vector\n",
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assessing Model\n",
    "\n",
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_demo_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_demo_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's count how each document ranks with respect to the training corpus\n",
    "collections.Counter(ranks)  # Results vary between runs due to random seeding and very small corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Testing the Model\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = []\n",
    "\n",
    "for doc_id in range(32400):\n",
    "    reg_list_real=[]\n",
    "    reg_list_ideal=[]\n",
    "    \n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    #print (sims)\n",
    "    a, b = zip(*sims)\n",
    "    \n",
    "    \n",
    "    reg_list_real=list(b)\n",
    "    \n",
    "    max= 0\n",
    "    for i in reg_list_real:\n",
    "        if i > max:\n",
    "            max=i\n",
    "      \n",
    "    reg_list_ideal=[0] * 5\n",
    "    \n",
    "    reg_list_ideal.insert(0,100)\n",
    "    \n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    slope, intercept, r_value, p_value, std_err=stats.linregress(reg_list_real,reg_list_ideal)\n",
    "    #print(\"p_value: %f\" % (p_value))\n",
    "    \n",
    "    \n",
    "    d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),\"P Value\":p_value})\n",
    "\n",
    "    \n",
    "     \n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing the Model without P values\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = []\n",
    "\n",
    "for doc_id in range(874):\n",
    "    reg_list_real=[]\n",
    "    reg_list_ideal=[]\n",
    "    \n",
    "    inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    #print (sims)\n",
    "    a, b = zip(*sims)\n",
    "    \n",
    "    \n",
    "    d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5])})\n",
    "    #d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),a[6]:(b[6])})\n",
    "    #d.append({a[0]:(b[0])})\n",
    "    #d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),a[6]:(b[6]),a[7]:(b[7]),a[8]:(b[8]),a[9]:(b[9]),a[10]:(b[10]),a[11]:(b[11]),a[12]:(b[12]),a[13]:(b[13]),a[14]:(b[14]),a[15]:(b[15]),a[16]:(b[16]),a[17]:(b[17]),a[18]:(b[18]),a[19]:(b[19]),a[20]:(b[20]),a[21]:(b[21]),a[22]:(b[22]),a[23]:(b[23]),a[24]:(b[24]),a[25]:(b[25]),a[26]:(b[26]),a[27]:(b[27]),a[28]:(b[28]),a[29]:(b[29]),a[30]:(b[30]),a[31]:(b[31]),a[32]:(b[32]),a[33]:(b[33]),a[34]:(b[34]),a[35]:(b[35]),a[36]:(b[36]),a[37]:(b[37]),a[38]:(b[38]),a[39]:(b[39]),a[40]:(b[40]),a[41]:(b[41]),a[42]:(b[42]),a[43]:(b[43]),a[44]:(b[44]),a[45]:(b[45]),a[46]:(b[46]),a[47]:(b[47]),a[48]:(b[48]),a[49]:(b[49])})\n",
    "    #d.append({a[0]:(b[0]),a[1]:(b[1]),a[2]:(b[2]),a[3]:(b[3]),a[4]:(b[4]),a[5]:(b[5]),a[6]:(b[6]),a[7]:(b[7]),a[8]:(b[8]),a[9]:(b[9]),a[10]:(b[10]),a[11]:(b[11]),a[12]:(b[12]),a[13]:(b[13]),a[14]:(b[14]),a[15]:(b[15]),a[16]:(b[16]),a[17]:(b[17]),a[18]:(b[18]),a[19]:(b[19]),a[20]:(b[20]),a[21]:(b[21]),a[22]:(b[22]),a[23]:(b[23]),a[24]:(b[24]),a[25]:(b[25])}) \n",
    "def highlight_max(s):\n",
    "    '''\n",
    "    highlight the maximum in a Series yellow.\n",
    "    '''\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: yellow' if v else '' for v in is_max]\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export above table as csv file\n",
    "\n",
    "pd.DataFrame(d).style.apply(highlight_max,axis=1).to_excel(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/mymodel_15_lemma_train_6Fields_Test_CogSci2020.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be sure about the doc number and text matching\n",
    "for f in range(30):\n",
    "    print('Test Document ({}): «{}»\\n'.format(f, ' '.join(test_corpus[f][:10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train setin vektörleri\n",
    "X=[]\n",
    "for i in range(6):\n",
    "    X.append(model.docvecs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train set vektörlerini txt dosyasına yaz\n",
    "with open(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/Train6Fields_vectors.txt\", 'w') as f:\n",
    "    for item in X:\n",
    "        f.write(\"{0}\\n\".format(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test setin vektörleri\n",
    "Y=[]\n",
    "for i in range(874):\n",
    "    Y.append(model.infer_vector(test_corpus[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set vektörlerini txt dosyasına yaz\n",
    "\n",
    "with open(\"C:/Users/ENF_RP/Desktop/Journals/Doc2Vec/CogSCI/Revised_Study_After_TIK/New_Study_for_Conference_31_12_2019/Second_Iteration_Review/Test_CogSci2020_for_108KTrainingSet_vectors.txt\", 'w') as f:\n",
    "    for item in Y:\n",
    "        f.write(\"{0}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
